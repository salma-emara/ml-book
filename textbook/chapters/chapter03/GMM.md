# Gaussian Mixture Models (GMM)

**<span style="font-size:1.5em;">Motivation</span>**

Single Gaussian distributions are limited in representing complex or multi-modal data. To model more flexible distributions, we use a **mixture of Gaussians**, where data is assumed to be generated from one of several Gaussian components.

This leads to the **Gaussian Mixture Model (GMM)** — a probabilistic model that expresses the overall density as a weighted sum of multiple Gaussians.

<br>

**<span style="font-size:1.5em;">GMM Definition</span>**

A GMM with $M$ components models the data distribution as:

$$
P(\mathbf{x}) = \sum_{m=1}^M \pi_m \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m)
$$

Where:

- $\pi_m$ is the **mixing coefficient** for component $m$, such that $\sum_{m=1}^M \pi_m = 1$
- $\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m)$ is a multivariate Gaussian with mean $\boldsymbol{\mu}_m$ and covariance $\boldsymbol{\Sigma}_m$

<br>

**<span style="font-size:1.5em;">Latent Variable Interpretation</span>**

GMMs can be viewed as a latent variable model:

- Introduce a latent variable $z \in \{1, \dots, M\}$ indicating which Gaussian generated each point.
- The **joint distribution** becomes:

$$
P(\mathbf{x}, z = m) = \pi_m \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m)
$$

- The **marginal** over $x$ recovers the GMM:

$$
P(\mathbf{x}) = \sum_{m=1}^M P(\mathbf{x}, z = m)
$$

<br>

**<span style="font-size:1.5em;">Parameter Learning</span>**

To fit a GMM to data, we estimate:

- Mixing weights $\pi_m$
- Means $\boldsymbol{\mu}_m$
- Covariances $\boldsymbol{\Sigma}_m$

The standard algorithm is **Expectation-Maximization (EM)**, which alternates:

1. **E-step**: Compute responsibilities $r_{n,m}$ — the probability that data point $\mathbf{x}_n$ was generated by component $m$:

$$
r_{n,m} = \frac{\pi_m \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m)}{\sum_{j=1}^M \pi_j \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
$$

2. **M-step**: Update parameters using these responsibilities:

$$
\pi_m^{\text{new}} = \frac{1}{N} \sum_{n=1}^N r_{n,m}
$$

$$
\boldsymbol{\mu}_m^{\text{new}} = \frac{\sum_{n=1}^N r_{n,m} \mathbf{x}_n}{\sum_{n=1}^N r_{n,m}}
$$

$$
\boldsymbol{\Sigma}_m^{\text{new}} = \frac{\sum_{n=1}^N r_{n,m} (\mathbf{x}_n - \boldsymbol{\mu}_m)(\mathbf{x}_n - \boldsymbol{\mu}_m)^\top}{\sum_{n=1}^N r_{n,m}}
$$

Repeat E and M steps until convergence.

<br>

**<span style="font-size:1.5em;">Applications</span>**

GMMs are widely used for:

- **Clustering** (soft clustering alternative to $K$-means)
- **Density estimation**
- **Anomaly detection**
- **Data imputation**
- **Speech and image processing**

<br>

**<span style="font-size:1.5em;">Relation to K-Means</span>**

GMMs generalize $K$-means:

- $K$-means assigns each point to exactly one cluster (hard assignment).
- GMMs use **soft assignments** via probabilistic responsibilities.
- $K$-means can be viewed as a special case of GMM where:
  - Covariances are identity matrices.
  - Mixing coefficients are uniform.
  - Responsibilities are binary (0 or 1).

<br>

**<span style="font-size:1.5em;">Summary</span>**

- GMMs model data as a weighted sum of Gaussians.
- They handle complex, non-convex, and overlapping clusters.
- Learning is done using the EM algorithm.
- Provide a powerful probabilistic alternative to hard clustering methods.

