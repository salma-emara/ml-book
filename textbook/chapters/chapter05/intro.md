# 5. Linear Regression and Classification

This chapter introduces two fundamental approaches within the supervised learning paradigm: **linear regression** for predicting continuous-valued outputs and **linear classification** for assigning samples to discrete categories. Both methods rely on linear models, wherein the prediction is obtained as a linear function of the input features.

We begin with the formal **problem formulation** of supervised learning, including definitions of the hypothesis set, labeled datasets, loss functions, and the concepts of risk and empirical risk. The derivation of learning algorithms is presented through the principle of empirical risk minimization.

For **linear regression**, we consider both scalar and vector outputs, derive the closed-form solution to the minimization problem, discuss convexity properties, and extend the model to affine transformations. Examples such as polynomial curve fitting and line fitting are provided to illustrate the concepts.

For **linear classification**, we examine binary decision rules based on linear separation, including the simple case of threshold-based separation in one dimension. The use of loss functions for classification, such as the 0â€“1 loss, is discussed in the context of both empirical and exact risk.

The chapter also includes a **geometric interpretation** of linear models, relating them to hyperplanes in feature space, and concludes with a discussion of **evaluation metrics** for regression and classification performance assessment.
