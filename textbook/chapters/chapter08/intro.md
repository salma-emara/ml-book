# Neural Networks

**<span style="font-size:1.5em;">Introduction</span>**

Neural Networks (NNs) represent a central paradigm in modern machine learning.  
They extend linear models by introducing nonlinear transformations, enabling us to capture highly complex structures in data.  
From their biological inspiration to their mathematical formulation, neural networks provide a framework that is both expressive and flexible.

In this chapter, we will explore:

- The biological and historical motivation behind neural networks  
- The perceptron model and its limitations  
- Multilayer perceptrons (MLPs) and the principle of forward propagation  
- How neural networks are trained, including backpropagation and optimization techniques  
- Design considerations such as depth, normalization, and regularization  
- The expressive power of NNs, including their role in universal approximation and the challenges of overfitting  

This chapter establishes the foundation for understanding *deep learning*: why depth matters, how models are trained effectively, and how neural networks achieve state-of-the-art performance across many tasks.
