# Gaussian Mixture Models (GMMs)

**<span style="font-size:1.5em;">Gaussian Mixture Models (GMMs)</span>**

Gaussian Mixture Models approximate a complex data distribution by combining multiple Gaussian components.  
A GMM assumes that each sample $x$ is generated by choosing a component $k$ and then drawing from its Gaussian distribution.

The model is defined as:

$$
p_{\theta}(x) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \Sigma_k)
$$

where  
- $\pi_k$ are mixing coefficients with $\sum_k \pi_k = 1$,  
- $\mu_k$ and $\Sigma_k$ are the mean and covariance of component $k$.

Learning a GMM means choosing parameters

$$
\theta = \{\pi_k, \mu_k, \Sigma_k\}_{k=1}^{K}
$$

that maximize the likelihood of data:

$$
\theta^{*} = \arg\max_{\theta} \sum_{i=1}^{N} \log p_{\theta}(x^{(i)})
$$

Since direct optimization is hard, the Expectationâ€“Maximization (EM) algorithm is often used:

**E-step:** Compute soft assignments (responsibilities):

$$
r_{ik} = 
\frac{\pi_k \, \mathcal{N}(x^{(i)} \mid \mu_k, \Sigma_k)}
{\sum_{j} \pi_j \, \mathcal{N}(x^{(i)} \mid \mu_j, \Sigma_j)}
$$

**M-step:** Update parameters using these responsibilities:

$$
\pi_k = \frac{1}{N} \sum_i r_{ik},
\qquad
\mu_k = \frac{\sum_i r_{ik} x^{(i)}}{\sum_i r_{ik}},
$$

$$
\Sigma_k = 
\frac{\sum_i r_{ik} (x^{(i)} - \mu_k)(x^{(i)} - \mu_k)^{\top}}
{\sum_i r_{ik}}
$$

Once trained, sampling from a GMM is simple:

1. Pick a component $k$ according to $\pi_k$  
2. Draw $x \sim \mathcal{N}(\mu_k, \Sigma_k)$  

GMMs provide a simple yet powerful baseline for density estimation and generative modeling.
