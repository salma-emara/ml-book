# Exploration vs. Exploitation Trade-off

**<span style="font-size:1.5em;">Exploration vs. Exploitation Trade-off</span>**

An RL agent must balance two behaviors:

- **Exploitation:** choosing the best-known action to maximize reward now  
- **Exploration:** trying new actions to discover potentially better long-term rewards

Focusing only on exploitation can trap the agent in suboptimal behavior.  
Focusing only on exploration wastes time on actions that are unlikely to help.  
Effective RL requires a balance between the two.

---

**ε-Greedy Strategy**

A common exploration rule is ε-greedy:

$$
a_t =
\begin{cases}
\text{random action} & \text{with probability } \varepsilon, \\
\arg\max_a Q(s_t, a) & \text{with probability } 1 - \varepsilon.
\end{cases}
$$

- Large $\varepsilon$ encourages more exploration  
- Small $\varepsilon$ focuses on exploitation  
- $\varepsilon$ is often decayed over time

---

**Uncertainty and Value Estimates**

The agent’s Q-values contain uncertainty early in learning.  
Exploration allows the agent to reduce uncertainty by observing new transitions:

$$
(s_t, a_t, r_{t+1}, s_{t+1})
$$

As the agent gathers experience, estimates become more reliable, making exploitation more effective.

---

**Trade-off Goal**

The objective is not to explore or exploit exclusively, but to **maximize long-term return**:

$$
\max_{\pi} \; \mathbb{E}[G_t]
$$

This requires exploring enough to identify good actions while exploiting known strong actions to accumulate reward.  
Efficient RL methods carefully control this balance throughout learning.
